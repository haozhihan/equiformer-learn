使用设备: cuda

加载 test 数据集...
数据集大小: 10831

创建模型: graph_attention_transformer_nonlinear_bessel_l2_drop00
type o3.Irreps('{}x0e'.format(self.max_atom_type))  5x0e
type self.irreps_node_embedding 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 128x0e+64x1e+32x2e
irreps_node_input 128x0e+64x1e+32x2e
irreps_node_attr 1x0e
irreps_edge_attr 1x0e+1x1e+1x2e
irreps_node_output 512x0e

加载 checkpoint: models/qm9/equiformer/se_l2/target@7/lr@1.5e-4_epochs@10_bs@64_wd@0.0_dropout@0.0_bessel@8_no-stad_l1-loss_g@4/best_checkpoint.pth.tar
✓ 成功加载 checkpoint (epoch: 9)
  Best val MAE: 0.05746
  Best test MAE: 0.05723

开始推理...
node_atom: tensor([3, 0, 0, 3, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        3, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1,
        3, 1, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 0, 0, 0, 1, 1, 2, 1, 3, 0, 0,
        0, 0, 0, 3, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 2, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 3,
        0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 3, 1, 2, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 3, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1,
        3, 1, 1, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 0, 0, 0, 1, 2, 1, 3, 1, 3, 0, 0,
        0, 0, 0, 0, 0, 1, 2, 1, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3,
        0, 0, 0, 3, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0,
        0, 0, 3, 1, 1, 3, 1, 3, 0, 0, 0, 0, 3, 1, 1, 3, 1, 3, 0, 0, 0, 0, 3, 1,
        1, 1, 1, 3, 0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 3, 2, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 3,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        3, 1, 3, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0,
        1, 2, 2, 2, 1, 2, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        ...,
        [0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([800, 5])
atom_type_lin's output torch.Size([800, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
self.norm_1 output  torch.Size([800, 480])
message in GraphAttention:  torch.Size([9306, 480])
node_atom: tensor([1, 2, 1, 2, 2, 1, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 0, 0, 0, 0, 2, 1, 1,
        1, 2, 3, 0, 0, 0, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, 3, 2, 3, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1, 0,
        0, 0, 0, 0, 4, 1, 4, 4, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 2, 0, 0,
        0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 1, 3, 0,
        0, 0, 0, 0, 3, 1, 2, 1, 3, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 3, 3, 0, 0, 0, 0, 0, 1, 1,
        2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 2,
        1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3,
        1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0,
        2, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0,
        0, 2, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0,
        0, 0, 0, 3, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2,
        1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 2, 1, 1, 3, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 3, 1, 2,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 1, 1,
        1, 2, 0, 0, 1, 1, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1,
        3, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1,
        1, 1, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1,
        3, 0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1,
        2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 2, 1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 2,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0,
        0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([866, 5])
atom_type_lin's output torch.Size([866, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
self.norm_1 output  torch.Size([866, 480])
message in GraphAttention:  torch.Size([10232, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1014, 5])
atom_type_lin's output torch.Size([1014, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
self.norm_1 output  torch.Size([1014, 480])
message in GraphAttention:  torch.Size([14364, 480])
node_atom: tensor([1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 3,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1,
        1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0,
        0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1,
        1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2,
        1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1,
        3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 1, 3, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 2, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1,
        1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0,
        0, 0, 3, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 2, 0,
        0, 0, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 3,
        0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 2, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
        1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0,
        0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 3, 0, 0, 0, 2, 1, 3, 1, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        3, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
        1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([940, 5])
atom_type_lin's output torch.Size([940, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
self.norm_1 output  torch.Size([940, 480])
message in GraphAttention:  torch.Size([12076, 480])
node_atom: tensor([3, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 3, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        1, 2, 1, 3, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 2, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 3, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3,
        1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 3, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 3, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 3, 1, 3, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 2, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 3, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 2, 1, 2, 2,
        2, 0, 0, 0, 0, 1, 1, 2, 2, 1, 3, 2, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 3,
        0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2,
        1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0, 1, 2,
        2, 2, 1, 1, 3, 0, 0, 0, 0, 0, 1, 2, 1, 3, 2, 2, 3, 0, 0, 0, 1, 2, 1, 1,
        1, 2, 3, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 1, 3, 1,
        1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 3, 1, 1, 0, 0, 2, 1, 1, 1, 3,
        2, 2, 0, 0, 0, 2, 1, 2, 2, 3, 2, 2, 0, 2, 1, 2, 2, 1, 2, 1, 0, 0, 0, 0,
        2, 1, 3, 2, 2, 1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 2, 1, 1,
        3, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 3, 0, 0, 0, 0, 2, 1, 2, 2,
        1, 2, 2, 0, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1, 2, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([882, 5])
atom_type_lin's output torch.Size([882, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
self.norm_1 output  torch.Size([882, 480])
message in GraphAttention:  torch.Size([11254, 480])
node_atom: tensor([3, 1, 1, 1, 3, 2, 2, 0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 2, 3, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 3, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 2,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 3, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 2, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2,
        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 3, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 2, 3, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 3, 2,
        1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 3, 2, 0, 0, 0, 0, 1, 1, 1, 1, 2,
        2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 0,
        1, 1, 2, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 3, 2, 1, 3, 0, 0,
        0, 1, 1, 2, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 2, 1, 2, 0,
        0, 0, 0, 0, 3, 1, 2, 1, 2, 3, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 2,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        2, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 2, 1, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 3, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1,
        3, 3, 0, 0, 0, 2, 1, 1, 2, 3, 1, 3, 2, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1,
        3, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 3, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 3, 0,
        0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 3, 3, 0, 0, 0, 0, 0, 2, 1, 1, 1,
        2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 3, 1, 2, 2, 0, 0, 0, 0, 0,
        0, 3, 1, 1, 1, 3, 1, 3, 2, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 2, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1,
        1, 3, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 2, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 2, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3, 2, 1, 0, 0, 0,
        0, 1, 2, 1, 2, 1, 3, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0,
        0, 0, 0, 1, 1, 1, 2, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1,
        2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 1, 3, 1, 0, 0, 0, 0, 0,
        0, 0, 2, 1, 1, 2, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 3,
        3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1,
        1, 2, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 3, 1,
        2, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([918, 5])
atom_type_lin's output torch.Size([918, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
self.norm_1 output  torch.Size([918, 480])
message in GraphAttention:  torch.Size([11204, 480])
node_atom: tensor([2, 1, 2, 1, 1, 3, 1, 3, 0, 0, 0, 0, 3, 1, 2, 1, 1, 3, 1, 3, 0, 0, 0, 1,
        1, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 3, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1,
        1, 3, 1, 3, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 1, 3,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3, 1, 3, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1,
        2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 2,
        0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1,
        1, 3, 0, 0, 0, 0, 3, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 1, 1,
        3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 1, 2, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,
        1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 2, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1,
        2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1,
        1, 3, 1, 2, 1, 2, 0, 0, 0, 3, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 1, 1, 1, 2,
        1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 1, 1, 3, 0, 0, 0, 0, 0, 3,
        1, 1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 2, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 2, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 2, 1,
        2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1,
        1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 3, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 3, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 2, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1,
        2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 3, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([957, 5])
atom_type_lin's output torch.Size([957, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
self.norm_1 output  torch.Size([957, 480])
message in GraphAttention:  torch.Size([12374, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1135, 5])
atom_type_lin's output torch.Size([1135, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
self.norm_1 output  torch.Size([1135, 480])
message in GraphAttention:  torch.Size([17036, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1200, 5])
atom_type_lin's output torch.Size([1200, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
self.norm_1 output  torch.Size([1200, 480])
message in GraphAttention:  torch.Size([18396, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1134, 5])
atom_type_lin's output torch.Size([1134, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
self.norm_1 output  torch.Size([1134, 480])
message in GraphAttention:  torch.Size([17716, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1105, 5])
atom_type_lin's output torch.Size([1105, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
self.norm_1 output  torch.Size([1105, 480])
message in GraphAttention:  torch.Size([17022, 480])
node_atom: tensor([2, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1046, 5])
atom_type_lin's output torch.Size([1046, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
self.norm_1 output  torch.Size([1046, 480])
message in GraphAttention:  torch.Size([14470, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1112, 5])
atom_type_lin's output torch.Size([1112, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16670, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1067, 5])
atom_type_lin's output torch.Size([1067, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
self.norm_1 output  torch.Size([1067, 480])
message in GraphAttention:  torch.Size([15934, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1103, 5])
atom_type_lin's output torch.Size([1103, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([16486, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1084, 5])
atom_type_lin's output torch.Size([1084, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
self.norm_1 output  torch.Size([1084, 480])
message in GraphAttention:  torch.Size([16626, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1112, 5])
atom_type_lin's output torch.Size([1112, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
self.norm_1 output  torch.Size([1112, 480])
message in GraphAttention:  torch.Size([16714, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1149, 5])
atom_type_lin's output torch.Size([1149, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
self.norm_1 output  torch.Size([1149, 480])
message in GraphAttention:  torch.Size([18028, 480])
node_atom: tensor([3, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1078, 5])
atom_type_lin's output torch.Size([1078, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([14714, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1082, 5])
atom_type_lin's output torch.Size([1082, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
self.norm_1 output  torch.Size([1082, 480])
message in GraphAttention:  torch.Size([15542, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1052, 5])
atom_type_lin's output torch.Size([1052, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
self.norm_1 output  torch.Size([1052, 480])
message in GraphAttention:  torch.Size([15620, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1063, 5])
atom_type_lin's output torch.Size([1063, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15872, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1073, 5])
atom_type_lin's output torch.Size([1073, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
self.norm_1 output  torch.Size([1073, 480])
message in GraphAttention:  torch.Size([16198, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1037, 5])
atom_type_lin's output torch.Size([1037, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
self.norm_1 output  torch.Size([1037, 480])
message in GraphAttention:  torch.Size([15360, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1069, 5])
atom_type_lin's output torch.Size([1069, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
self.norm_1 output  torch.Size([1069, 480])
message in GraphAttention:  torch.Size([16334, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1068, 5])
atom_type_lin's output torch.Size([1068, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
self.norm_1 output  torch.Size([1068, 480])
message in GraphAttention:  torch.Size([16120, 480])
node_atom: tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        2, 2, 1, 2, 2, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 2, 2, 2, 3, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 2, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 2, 2, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3,
        1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 3, 2, 2, 0, 0, 0, 1,
        3, 1, 1, 3, 1, 2, 3, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 2, 3, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2,
        2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 3, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 3, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 2,
        2, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 3, 2, 2, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 2, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3,
        2, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 1, 2, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 3, 2, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 2, 2, 2, 0, 0, 0, 0,
        1, 1, 2, 1, 1, 3, 2, 3, 0, 0, 0, 0, 1, 1, 2, 1, 1, 3, 2, 3, 0, 0, 0, 0,
        0, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 3, 1, 2, 0,
        0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 2, 3, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 2, 2, 3, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 3, 2, 2, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 3, 3, 2, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1,
        1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 3, 2, 2, 0, 0, 0, 0, 1, 2,
        2, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 1, 2, 0, 0, 0, 0,
        0, 0, 0, 1, 2, 2, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 3, 1,
        3, 0, 0, 0, 0, 0, 1, 2, 2, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
        2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([921, 5])
atom_type_lin's output torch.Size([921, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
self.norm_1 output  torch.Size([921, 480])
message in GraphAttention:  torch.Size([11166, 480])
node_atom: tensor([1, 2, 2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0,
        1, 2, 2, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 1, 1, 2, 0,
        0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 1, 2, 1,
        1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0,
        1, 2, 1, 3, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 2, 1, 3, 2, 1, 3, 3, 0, 0,
        0, 0, 1, 2, 1, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2,
        2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 2, 1, 1, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 3,
        1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 1, 2, 3, 0, 0, 0, 0,
        0, 0, 1, 3, 1, 1, 3, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 2, 2,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 1, 3, 1,
        2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 2, 3, 1, 2, 1, 0, 0, 0, 0,
        0, 0, 1, 3, 1, 2, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 3, 1, 2,
        0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 3, 3, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1, 3,
        0, 0, 0, 0, 2, 1, 3, 1, 3, 3, 2, 1, 0, 0, 2, 1, 3, 1, 2, 3, 2, 2, 0, 0,
        2, 1, 3, 1, 1, 1, 2, 3, 0, 0, 0, 0, 2, 1, 3, 1, 1, 3, 2, 2, 0, 0, 0, 2,
        1, 1, 1, 3, 1, 2, 3, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 2,
        1, 1, 2, 2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0,
        2, 1, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0,
        2, 1, 1, 2, 2, 1, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 1, 3, 0, 0, 0,
        0, 0, 0, 2, 1, 2, 1, 2, 2, 1, 2, 0, 0, 0, 2, 1, 2, 1, 3, 2, 2, 1, 0, 0,
        0, 0, 2, 1, 2, 1, 2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 2, 1, 2, 3, 1, 2, 0, 0,
        0, 0, 2, 1, 2, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 3, 0,
        0, 0, 0, 0, 2, 1, 2, 0, 1, 2, 1, 2, 2, 0, 0, 0, 0, 3, 1, 1, 3, 2, 2, 1,
        2, 0, 3, 1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 3, 1, 1, 1, 1, 3, 2, 1, 0, 0, 0,
        3, 2, 2, 1, 1, 1, 2, 3, 0, 3, 2, 1, 2, 2, 2, 2, 1, 0, 3, 1, 2, 2, 1, 2,
        1, 1, 0, 0, 0, 3, 1, 1, 2, 3, 1, 3, 2, 0, 0, 3, 1, 1, 1, 2, 3, 1, 2, 0,
        0, 0, 0, 3, 1, 3, 1, 1, 1, 2, 2, 0, 0, 0, 0, 3, 1, 3, 1, 1, 2, 2, 2, 0,
        0, 0, 3, 1, 3, 1, 2, 3, 2, 1, 0, 0, 3, 1, 1, 2, 1, 3, 2, 2, 0, 0, 0, 3,
        1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 3, 1, 2, 2, 1, 1, 1, 3, 0, 0, 0, 0, 3, 1,
        2, 3, 1, 2, 1, 2, 0, 0, 0, 3, 1, 2, 3, 1, 2, 1, 1, 0, 0],
       device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([811, 5])
atom_type_lin's output torch.Size([811, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
self.norm_1 output  torch.Size([811, 480])
message in GraphAttention:  torch.Size([8694, 480])
node_atom: tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1089, 5])
atom_type_lin's output torch.Size([1089, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
self.norm_1 output  torch.Size([1089, 480])
message in GraphAttention:  torch.Size([15624, 480])
node_atom: tensor([3, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1165, 5])
atom_type_lin's output torch.Size([1165, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
self.norm_1 output  torch.Size([1165, 480])
message in GraphAttention:  torch.Size([18030, 480])
node_atom: tensor([3, 2, 1, 1, 3, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 3,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1, 3, 1, 0, 0, 0,
        0, 0, 0, 0, 3, 2, 1, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 2, 1, 1, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2,
        3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1,
        1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0,
        0, 0, 0, 0, 3, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1,
        1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 2, 1, 1, 1, 2, 0,
        0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        3, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 3, 1,
        3, 1, 0, 0, 0, 0, 0, 4, 1, 2, 1, 3, 3, 1, 2, 0, 1, 1, 2, 1, 2, 1, 4, 1,
        0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 4, 1, 0, 0, 0, 0, 4, 1, 1, 1, 4, 2, 1,
        2, 0, 0, 4, 1, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 4, 1, 1, 2, 1, 4, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1,
        4, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 4, 0, 0, 0, 0, 0, 4, 1, 1, 2, 1,
        2, 1, 4, 0, 0, 4, 1, 1, 1, 1, 2, 1, 3, 0, 0, 0, 0, 4, 1, 1, 2, 1, 2, 1,
        3, 0, 0, 0, 4, 1, 1, 1, 3, 3, 2, 1, 0, 0, 4, 1, 1, 1, 2, 3, 1, 3, 0, 0,
        4, 1, 1, 2, 2, 2, 1, 3, 0, 0, 4, 1, 1, 3, 2, 1, 1, 3, 0, 0, 4, 1, 2, 3,
        1, 2, 1, 2, 0, 0, 4, 1, 2, 3, 1, 3, 1, 1, 0, 0, 4, 1, 2, 3, 1, 3, 2, 1,
        0, 4, 1, 2, 3, 1, 1, 1, 3, 0, 0, 4, 1, 2, 3, 2, 1, 1, 2, 0, 0, 1, 3, 1,
        1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 4, 1, 1, 1, 1, 0, 4, 1, 4,
        4, 1, 3, 1, 2, 0, 0, 0, 0, 3, 1, 1, 1, 1, 4, 4, 4, 0, 0, 0, 4, 1, 4, 4,
        1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 3, 1,
        2, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2,
        1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0,
        2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        3, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 2,
        1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        3, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([902, 5])
atom_type_lin's output torch.Size([902, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
self.norm_1 output  torch.Size([902, 480])
message in GraphAttention:  torch.Size([11112, 480])
node_atom: tensor([1, 1, 2, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 3, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 3, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 2,
        1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 1, 1, 2, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 3, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 3, 1, 2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 3, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 3, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2,
        1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0,
        0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1,
        2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 2,
        1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 1, 2, 1, 0, 0,
        0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2,
        1, 2, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 3, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 1,
        1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2,
        1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0,
        1, 1, 1, 2, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2,
        1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 0, 0,
        0, 0, 0, 0, 2, 1, 2, 1, 3, 2, 1, 2, 1, 0, 0, 0, 0, 3, 1, 2, 1, 3, 1, 2,
        1, 1, 0, 0, 0, 0, 3, 1, 3, 1, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 2,
        2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 3, 1,
        1, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 3, 1, 1, 2, 0, 0, 0,
        0, 0, 3, 1, 2, 1, 1, 1, 1, 3, 3, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2, 1, 1,
        2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 3,
        1, 3, 1, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 3, 1, 1, 1, 2, 1, 1, 0, 0,
        0, 0, 2, 1, 3, 1, 1, 2, 1, 1, 2, 0, 0, 0, 3, 1, 3, 1, 1, 1, 1, 1, 2, 0,
        0, 0, 3, 1, 3, 1, 1, 1, 1, 2, 3, 0, 0, 0, 3, 1, 3, 1, 1, 2, 1, 1, 2, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3,
        1, 3, 2, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 2, 1, 2, 0, 0, 0, 0, 2, 1, 2,
        1, 2, 1, 3, 1, 2, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 3, 2, 1, 0, 0, 0, 0, 2,
        1, 3, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([949, 5])
atom_type_lin's output torch.Size([949, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
self.norm_1 output  torch.Size([949, 480])
message in GraphAttention:  torch.Size([11996, 480])
node_atom: tensor([3, 1, 3, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 3, 1, 2, 1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 1, 1,
        2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1,
        1, 1, 1, 2, 1, 3, 0, 0, 0, 3, 1, 1, 1, 3, 1, 2, 1, 1, 0, 0, 0, 3, 1, 1,
        1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3, 2, 1, 2, 0, 0, 0, 3, 1,
        1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 3, 1, 2, 0, 0, 2,
        1, 3, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 3, 2, 0, 0, 0,
        0, 0, 3, 1, 1, 1, 2, 1, 1, 3, 2, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 1, 3,
        0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 3, 2, 1, 0, 0, 0, 0, 0, 3, 1, 1, 1, 3, 1,
        1, 1, 1, 0, 0, 0, 0, 3, 1, 1, 1, 3, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 3,
        1, 1, 1, 3, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0, 3, 1, 2, 1,
        2, 1, 1, 1, 2, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 3,
        1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 2, 1,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 2,
        1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 1, 1, 3, 0, 0, 0,
        0, 0, 0, 3, 1, 1, 2, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 2, 1, 3, 1, 1, 2, 1,
        1, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0,
        3, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 1, 1, 1,
        2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1,
        1, 3, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 3, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2,
        1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3,
        1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 3, 2, 1, 3, 0,
        0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 1, 3, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1, 3, 1, 2, 0, 0, 0,
        0, 0, 0, 1, 1, 3, 1, 1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1,
        1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1, 1, 2, 2, 0, 0, 0, 0, 0,
        0, 0, 2, 1, 3, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 2, 1, 3, 1, 1, 2, 1,
        3, 2, 0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 2, 1, 2, 0, 0, 2, 0, 0, 0, 1, 1, 1,
        1, 1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 3, 1, 1, 3, 0,
        0, 0, 0, 0, 0, 2, 1, 3, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3,
        2, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 2, 3, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 3, 2, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 3,
        1, 2, 1, 1, 3, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0,
        0, 2, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1,
        1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0],
       device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([934, 5])
atom_type_lin's output torch.Size([934, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
self.norm_1 output  torch.Size([934, 480])
message in GraphAttention:  torch.Size([10976, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1095, 5])
atom_type_lin's output torch.Size([1095, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
self.norm_1 output  torch.Size([1095, 480])
message in GraphAttention:  torch.Size([15454, 480])
node_atom: tensor([1, 1, 1, 2, 3, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 1, 3, 1, 3,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 3, 2,
        0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 1, 2, 3, 0, 0, 0, 0, 0, 2, 1, 1, 3,
        2, 1, 1, 3, 2, 0, 0, 0, 0, 0, 2, 1, 1, 3, 3, 1, 3, 1, 1, 0, 0, 0, 0, 0,
        3, 1, 1, 3, 2, 1, 1, 2, 3, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 3, 3, 1, 1, 0, 0, 0, 0, 1, 1, 1, 3,
        1, 3, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 3, 3, 1, 3, 0, 0, 0, 1, 1,
        1, 1, 1, 3, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 2, 1, 2, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1,
        3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 2, 3, 1, 2, 0, 0, 0, 0, 0,
        0, 2, 1, 1, 2, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 3, 3, 1,
        2, 0, 0, 0, 0, 1, 1, 1, 3, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 1, 1, 3, 0,
        0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 3, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 3, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2,
        1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0,
        2, 1, 1, 3, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 3, 1, 2, 1, 2,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 3, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 3, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 2, 1, 2, 2, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,
        2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 2, 1, 1, 2, 0, 0, 0, 0,
        0, 2, 1, 2, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 2,
        1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 2, 1, 2, 3, 0, 0, 0, 0,
        0, 3, 1, 1, 1, 2, 2, 1, 3, 3, 0, 0, 0, 0, 3, 1, 1, 1, 2, 3, 1, 3, 3, 0,
        0, 0, 3, 1, 2, 1, 3, 2, 1, 2, 3, 0, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 3, 1, 2, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 1,
        1, 1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1,
        2, 1, 2, 2, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 1, 1, 3, 2, 0, 0, 0, 0, 1, 1,
        1, 2, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 2, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 3, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([968, 5])
atom_type_lin's output torch.Size([968, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
self.norm_1 output  torch.Size([968, 480])
message in GraphAttention:  torch.Size([12070, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1002, 5])
atom_type_lin's output torch.Size([1002, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
self.norm_1 output  torch.Size([1002, 480])
message in GraphAttention:  torch.Size([12516, 480])
node_atom: tensor([1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1,
        3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 3, 2, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,
        1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3,
        1, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 1,
        3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 2, 1, 2, 1, 3, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 3, 1, 2,
        0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 1, 3, 3, 0, 0, 0, 0, 2, 1, 2, 1,
        1, 2, 1, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 2, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3,
        1, 1, 2, 0, 0, 0, 0, 3, 1, 1, 2, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1,
        2, 1, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        2, 1, 1, 3, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1, 3, 1, 1, 0, 0, 0,
        0, 0, 0, 2, 1, 1, 2, 1, 1, 3, 1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 3,
        1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 3, 1, 2, 0, 0, 0, 0, 3, 1, 2, 1,
        1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 3, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1,
        1, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 2, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 1, 1, 3, 0,
        0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1,
        1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0,
        0, 0, 2, 1, 1, 1, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 1, 1,
        3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2,
        1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 1, 3, 0, 0, 0, 0,
        3, 1, 2, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0, 1, 1, 1, 3, 1, 3, 1, 3, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 2, 1, 3, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3,
        1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 3, 1, 2, 2, 0, 0, 0, 0, 0,
        2, 1, 2, 1, 3, 1, 3, 1, 3, 0, 0, 0, 0, 1, 1, 1, 2, 1, 3, 1, 1, 3, 0, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3,
        1, 3, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0,
        0, 3, 1, 1, 3, 1, 3, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 3, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 3, 1, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 2, 2,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0,
        0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([986, 5])
atom_type_lin's output torch.Size([986, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
self.norm_1 output  torch.Size([986, 480])
message in GraphAttention:  torch.Size([12370, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1043, 5])
atom_type_lin's output torch.Size([1043, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
self.norm_1 output  torch.Size([1043, 480])
message in GraphAttention:  torch.Size([13916, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1103, 5])
atom_type_lin's output torch.Size([1103, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
self.norm_1 output  torch.Size([1103, 480])
message in GraphAttention:  torch.Size([15296, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1102, 5])
atom_type_lin's output torch.Size([1102, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
self.norm_1 output  torch.Size([1102, 480])
message in GraphAttention:  torch.Size([14692, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1110, 5])
atom_type_lin's output torch.Size([1110, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
self.norm_1 output  torch.Size([1110, 480])
message in GraphAttention:  torch.Size([15036, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1075, 5])
atom_type_lin's output torch.Size([1075, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
self.norm_1 output  torch.Size([1075, 480])
message in GraphAttention:  torch.Size([14050, 480])
node_atom: tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1038, 5])
atom_type_lin's output torch.Size([1038, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14952, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1063, 5])
atom_type_lin's output torch.Size([1063, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
self.norm_1 output  torch.Size([1063, 480])
message in GraphAttention:  torch.Size([15544, 480])
node_atom: tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1099, 5])
atom_type_lin's output torch.Size([1099, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
self.norm_1 output  torch.Size([1099, 480])
message in GraphAttention:  torch.Size([16426, 480])
node_atom: tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1123, 5])
atom_type_lin's output torch.Size([1123, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16664, 480])
node_atom: tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1172, 5])
atom_type_lin's output torch.Size([1172, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
self.norm_1 output  torch.Size([1172, 480])
message in GraphAttention:  torch.Size([17936, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1160, 5])
atom_type_lin's output torch.Size([1160, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
self.norm_1 output  torch.Size([1160, 480])
message in GraphAttention:  torch.Size([19418, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1201, 5])
atom_type_lin's output torch.Size([1201, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
self.norm_1 output  torch.Size([1201, 480])
message in GraphAttention:  torch.Size([20600, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1228, 5])
atom_type_lin's output torch.Size([1228, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21278, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1209, 5])
atom_type_lin's output torch.Size([1209, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
self.norm_1 output  torch.Size([1209, 480])
message in GraphAttention:  torch.Size([20152, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1250, 5])
atom_type_lin's output torch.Size([1250, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
self.norm_1 output  torch.Size([1250, 480])
message in GraphAttention:  torch.Size([21074, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1228, 5])
atom_type_lin's output torch.Size([1228, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([21566, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1074, 5])
atom_type_lin's output torch.Size([1074, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
self.norm_1 output  torch.Size([1074, 480])
message in GraphAttention:  torch.Size([15938, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1018, 5])
atom_type_lin's output torch.Size([1018, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
self.norm_1 output  torch.Size([1018, 480])
message in GraphAttention:  torch.Size([14770, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1012, 5])
atom_type_lin's output torch.Size([1012, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([14352, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1033, 5])
atom_type_lin's output torch.Size([1033, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
self.norm_1 output  torch.Size([1033, 480])
message in GraphAttention:  torch.Size([15096, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1044, 5])
atom_type_lin's output torch.Size([1044, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([15536, 480])
node_atom: tensor([2, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1038, 5])
atom_type_lin's output torch.Size([1038, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
self.norm_1 output  torch.Size([1038, 480])
message in GraphAttention:  torch.Size([14936, 480])
node_atom: tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1090, 5])
atom_type_lin's output torch.Size([1090, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
self.norm_1 output  torch.Size([1090, 480])
message in GraphAttention:  torch.Size([16674, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1108, 5])
atom_type_lin's output torch.Size([1108, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
self.norm_1 output  torch.Size([1108, 480])
message in GraphAttention:  torch.Size([17222, 480])
node_atom: tensor([3, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1012, 5])
atom_type_lin's output torch.Size([1012, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
self.norm_1 output  torch.Size([1012, 480])
message in GraphAttention:  torch.Size([13654, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1030, 5])
atom_type_lin's output torch.Size([1030, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
self.norm_1 output  torch.Size([1030, 480])
message in GraphAttention:  torch.Size([14976, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1021, 5])
atom_type_lin's output torch.Size([1021, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
self.norm_1 output  torch.Size([1021, 480])
message in GraphAttention:  torch.Size([14386, 480])
node_atom: tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1078, 5])
atom_type_lin's output torch.Size([1078, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
self.norm_1 output  torch.Size([1078, 480])
message in GraphAttention:  torch.Size([16026, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1101, 5])
atom_type_lin's output torch.Size([1101, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([16038, 480])
node_atom: tensor([3, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1109, 5])
atom_type_lin's output torch.Size([1109, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
self.norm_1 output  torch.Size([1109, 480])
message in GraphAttention:  torch.Size([14890, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1193, 5])
atom_type_lin's output torch.Size([1193, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
self.norm_1 output  torch.Size([1193, 480])
message in GraphAttention:  torch.Size([15702, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1176, 5])
atom_type_lin's output torch.Size([1176, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([15892, 480])
node_atom: tensor([2, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1085, 5])
atom_type_lin's output torch.Size([1085, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
self.norm_1 output  torch.Size([1085, 480])
message in GraphAttention:  torch.Size([15968, 480])
node_atom: tensor([2, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1141, 5])
atom_type_lin's output torch.Size([1141, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
self.norm_1 output  torch.Size([1141, 480])
message in GraphAttention:  torch.Size([16832, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1123, 5])
atom_type_lin's output torch.Size([1123, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
self.norm_1 output  torch.Size([1123, 480])
message in GraphAttention:  torch.Size([16284, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1285, 5])
atom_type_lin's output torch.Size([1285, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22406, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1284, 5])
atom_type_lin's output torch.Size([1284, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
self.norm_1 output  torch.Size([1284, 480])
message in GraphAttention:  torch.Size([21200, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1351, 5])
atom_type_lin's output torch.Size([1351, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
self.norm_1 output  torch.Size([1351, 480])
message in GraphAttention:  torch.Size([24956, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1362, 5])
atom_type_lin's output torch.Size([1362, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
self.norm_1 output  torch.Size([1362, 480])
message in GraphAttention:  torch.Size([24396, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1385, 5])
atom_type_lin's output torch.Size([1385, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
self.norm_1 output  torch.Size([1385, 480])
message in GraphAttention:  torch.Size([23484, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1156, 5])
atom_type_lin's output torch.Size([1156, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
self.norm_1 output  torch.Size([1156, 480])
message in GraphAttention:  torch.Size([17930, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1216, 5])
atom_type_lin's output torch.Size([1216, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
self.norm_1 output  torch.Size([1216, 480])
message in GraphAttention:  torch.Size([18990, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1161, 5])
atom_type_lin's output torch.Size([1161, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
self.norm_1 output  torch.Size([1161, 480])
message in GraphAttention:  torch.Size([18856, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1275, 5])
atom_type_lin's output torch.Size([1275, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
self.norm_1 output  torch.Size([1275, 480])
message in GraphAttention:  torch.Size([22440, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1325, 5])
atom_type_lin's output torch.Size([1325, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
self.norm_1 output  torch.Size([1325, 480])
message in GraphAttention:  torch.Size([24782, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1177, 5])
atom_type_lin's output torch.Size([1177, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
self.norm_1 output  torch.Size([1177, 480])
message in GraphAttention:  torch.Size([19122, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1181, 5])
atom_type_lin's output torch.Size([1181, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
self.norm_1 output  torch.Size([1181, 480])
message in GraphAttention:  torch.Size([18902, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1125, 5])
atom_type_lin's output torch.Size([1125, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
self.norm_1 output  torch.Size([1125, 480])
message in GraphAttention:  torch.Size([17746, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1098, 5])
atom_type_lin's output torch.Size([1098, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
self.norm_1 output  torch.Size([1098, 480])
message in GraphAttention:  torch.Size([17230, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1219, 5])
atom_type_lin's output torch.Size([1219, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
self.norm_1 output  torch.Size([1219, 480])
message in GraphAttention:  torch.Size([20880, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1155, 5])
atom_type_lin's output torch.Size([1155, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
self.norm_1 output  torch.Size([1155, 480])
message in GraphAttention:  torch.Size([18870, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1198, 5])
atom_type_lin's output torch.Size([1198, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
self.norm_1 output  torch.Size([1198, 480])
message in GraphAttention:  torch.Size([19632, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1225, 5])
atom_type_lin's output torch.Size([1225, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([20912, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1174, 5])
atom_type_lin's output torch.Size([1174, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
self.norm_1 output  torch.Size([1174, 480])
message in GraphAttention:  torch.Size([19294, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1164, 5])
atom_type_lin's output torch.Size([1164, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
self.norm_1 output  torch.Size([1164, 480])
message in GraphAttention:  torch.Size([19230, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1224, 5])
atom_type_lin's output torch.Size([1224, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
self.norm_1 output  torch.Size([1224, 480])
message in GraphAttention:  torch.Size([21492, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1191, 5])
atom_type_lin's output torch.Size([1191, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([20246, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1153, 5])
atom_type_lin's output torch.Size([1153, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
self.norm_1 output  torch.Size([1153, 480])
message in GraphAttention:  torch.Size([18138, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1190, 5])
atom_type_lin's output torch.Size([1190, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
self.norm_1 output  torch.Size([1190, 480])
message in GraphAttention:  torch.Size([18798, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1195, 5])
atom_type_lin's output torch.Size([1195, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
self.norm_1 output  torch.Size([1195, 480])
message in GraphAttention:  torch.Size([19782, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1273, 5])
atom_type_lin's output torch.Size([1273, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
self.norm_1 output  torch.Size([1273, 480])
message in GraphAttention:  torch.Size([22302, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1187, 5])
atom_type_lin's output torch.Size([1187, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
self.norm_1 output  torch.Size([1187, 480])
message in GraphAttention:  torch.Size([19670, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1229, 5])
atom_type_lin's output torch.Size([1229, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
self.norm_1 output  torch.Size([1229, 480])
message in GraphAttention:  torch.Size([20518, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1169, 5])
atom_type_lin's output torch.Size([1169, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
self.norm_1 output  torch.Size([1169, 480])
message in GraphAttention:  torch.Size([19384, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1159, 5])
atom_type_lin's output torch.Size([1159, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
self.norm_1 output  torch.Size([1159, 480])
message in GraphAttention:  torch.Size([19060, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1246, 5])
atom_type_lin's output torch.Size([1246, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
self.norm_1 output  torch.Size([1246, 480])
message in GraphAttention:  torch.Size([21784, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1236, 5])
atom_type_lin's output torch.Size([1236, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
self.norm_1 output  torch.Size([1236, 480])
message in GraphAttention:  torch.Size([21166, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1232, 5])
atom_type_lin's output torch.Size([1232, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
self.norm_1 output  torch.Size([1232, 480])
message in GraphAttention:  torch.Size([21356, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1245, 5])
atom_type_lin's output torch.Size([1245, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
self.norm_1 output  torch.Size([1245, 480])
message in GraphAttention:  torch.Size([22036, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1197, 5])
atom_type_lin's output torch.Size([1197, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([20160, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1196, 5])
atom_type_lin's output torch.Size([1196, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
self.norm_1 output  torch.Size([1196, 480])
message in GraphAttention:  torch.Size([19766, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1176, 5])
atom_type_lin's output torch.Size([1176, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
self.norm_1 output  torch.Size([1176, 480])
message in GraphAttention:  torch.Size([19062, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1295, 5])
atom_type_lin's output torch.Size([1295, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([22614, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1225, 5])
atom_type_lin's output torch.Size([1225, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
self.norm_1 output  torch.Size([1225, 480])
message in GraphAttention:  torch.Size([19908, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1239, 5])
atom_type_lin's output torch.Size([1239, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
self.norm_1 output  torch.Size([1239, 480])
message in GraphAttention:  torch.Size([20724, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1277, 5])
atom_type_lin's output torch.Size([1277, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
self.norm_1 output  torch.Size([1277, 480])
message in GraphAttention:  torch.Size([20872, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1295, 5])
atom_type_lin's output torch.Size([1295, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
self.norm_1 output  torch.Size([1295, 480])
message in GraphAttention:  torch.Size([21846, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1228, 5])
atom_type_lin's output torch.Size([1228, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
self.norm_1 output  torch.Size([1228, 480])
message in GraphAttention:  torch.Size([20586, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1248, 5])
atom_type_lin's output torch.Size([1248, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
self.norm_1 output  torch.Size([1248, 480])
message in GraphAttention:  torch.Size([21726, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1249, 5])
atom_type_lin's output torch.Size([1249, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
self.norm_1 output  torch.Size([1249, 480])
message in GraphAttention:  torch.Size([21840, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1274, 5])
atom_type_lin's output torch.Size([1274, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
self.norm_1 output  torch.Size([1274, 480])
message in GraphAttention:  torch.Size([22434, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1213, 5])
atom_type_lin's output torch.Size([1213, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([20280, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1271, 5])
atom_type_lin's output torch.Size([1271, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1271, 480])
message in GraphAttention:  torch.Size([22064, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1285, 5])
atom_type_lin's output torch.Size([1285, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
self.norm_1 output  torch.Size([1285, 480])
message in GraphAttention:  torch.Size([22822, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1339, 5])
atom_type_lin's output torch.Size([1339, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
self.norm_1 output  torch.Size([1339, 480])
message in GraphAttention:  torch.Size([24330, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1233, 5])
atom_type_lin's output torch.Size([1233, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
self.norm_1 output  torch.Size([1233, 480])
message in GraphAttention:  torch.Size([20280, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1137, 5])
atom_type_lin's output torch.Size([1137, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
self.norm_1 output  torch.Size([1137, 480])
message in GraphAttention:  torch.Size([14730, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1197, 5])
atom_type_lin's output torch.Size([1197, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
self.norm_1 output  torch.Size([1197, 480])
message in GraphAttention:  torch.Size([16480, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1191, 5])
atom_type_lin's output torch.Size([1191, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
self.norm_1 output  torch.Size([1191, 480])
message in GraphAttention:  torch.Size([16896, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1251, 5])
atom_type_lin's output torch.Size([1251, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([20644, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1324, 5])
atom_type_lin's output torch.Size([1324, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
self.norm_1 output  torch.Size([1324, 480])
message in GraphAttention:  torch.Size([22814, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1382, 5])
atom_type_lin's output torch.Size([1382, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
self.norm_1 output  torch.Size([1382, 480])
message in GraphAttention:  torch.Size([24692, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1305, 5])
atom_type_lin's output torch.Size([1305, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
self.norm_1 output  torch.Size([1305, 480])
message in GraphAttention:  torch.Size([21610, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1281, 5])
atom_type_lin's output torch.Size([1281, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
self.norm_1 output  torch.Size([1281, 480])
message in GraphAttention:  torch.Size([21310, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1288, 5])
atom_type_lin's output torch.Size([1288, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
self.norm_1 output  torch.Size([1288, 480])
message in GraphAttention:  torch.Size([22506, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1355, 5])
atom_type_lin's output torch.Size([1355, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
self.norm_1 output  torch.Size([1355, 480])
message in GraphAttention:  torch.Size([25124, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1304, 5])
atom_type_lin's output torch.Size([1304, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
self.norm_1 output  torch.Size([1304, 480])
message in GraphAttention:  torch.Size([23370, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1222, 5])
atom_type_lin's output torch.Size([1222, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
self.norm_1 output  torch.Size([1222, 480])
message in GraphAttention:  torch.Size([20422, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1314, 5])
atom_type_lin's output torch.Size([1314, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
self.norm_1 output  torch.Size([1314, 480])
message in GraphAttention:  torch.Size([23152, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1266, 5])
atom_type_lin's output torch.Size([1266, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
self.norm_1 output  torch.Size([1266, 480])
message in GraphAttention:  torch.Size([21390, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1251, 5])
atom_type_lin's output torch.Size([1251, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
self.norm_1 output  torch.Size([1251, 480])
message in GraphAttention:  torch.Size([21480, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1241, 5])
atom_type_lin's output torch.Size([1241, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
self.norm_1 output  torch.Size([1241, 480])
message in GraphAttention:  torch.Size([20172, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1292, 5])
atom_type_lin's output torch.Size([1292, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
self.norm_1 output  torch.Size([1292, 480])
message in GraphAttention:  torch.Size([22464, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1269, 5])
atom_type_lin's output torch.Size([1269, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
self.norm_1 output  torch.Size([1269, 480])
message in GraphAttention:  torch.Size([21496, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1289, 5])
atom_type_lin's output torch.Size([1289, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
self.norm_1 output  torch.Size([1289, 480])
message in GraphAttention:  torch.Size([22100, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1213, 5])
atom_type_lin's output torch.Size([1213, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
self.norm_1 output  torch.Size([1213, 480])
message in GraphAttention:  torch.Size([19948, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1332, 5])
atom_type_lin's output torch.Size([1332, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
self.norm_1 output  torch.Size([1332, 480])
message in GraphAttention:  torch.Size([22910, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1323, 5])
atom_type_lin's output torch.Size([1323, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
self.norm_1 output  torch.Size([1323, 480])
message in GraphAttention:  torch.Size([22178, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1293, 5])
atom_type_lin's output torch.Size([1293, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22064, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1293, 5])
atom_type_lin's output torch.Size([1293, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
self.norm_1 output  torch.Size([1293, 480])
message in GraphAttention:  torch.Size([22000, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1315, 5])
atom_type_lin's output torch.Size([1315, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
self.norm_1 output  torch.Size([1315, 480])
message in GraphAttention:  torch.Size([22824, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1256, 5])
atom_type_lin's output torch.Size([1256, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
self.norm_1 output  torch.Size([1256, 480])
message in GraphAttention:  torch.Size([17496, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1329, 5])
atom_type_lin's output torch.Size([1329, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
self.norm_1 output  torch.Size([1329, 480])
message in GraphAttention:  torch.Size([21474, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1407, 5])
atom_type_lin's output torch.Size([1407, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
self.norm_1 output  torch.Size([1407, 480])
message in GraphAttention:  torch.Size([24058, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1350, 5])
atom_type_lin's output torch.Size([1350, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
self.norm_1 output  torch.Size([1350, 480])
message in GraphAttention:  torch.Size([22872, 480])
node_atom: tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1330, 5])
atom_type_lin's output torch.Size([1330, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
self.norm_1 output  torch.Size([1330, 480])
message in GraphAttention:  torch.Size([21812, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1372, 5])
atom_type_lin's output torch.Size([1372, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
self.norm_1 output  torch.Size([1372, 480])
message in GraphAttention:  torch.Size([22720, 480])
node_atom: tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1374, 5])
atom_type_lin's output torch.Size([1374, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
self.norm_1 output  torch.Size([1374, 480])
message in GraphAttention:  torch.Size([21936, 480])
node_atom: tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1402, 5])
atom_type_lin's output torch.Size([1402, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
self.norm_1 output  torch.Size([1402, 480])
message in GraphAttention:  torch.Size([22360, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1101, 5])
atom_type_lin's output torch.Size([1101, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
self.norm_1 output  torch.Size([1101, 480])
message in GraphAttention:  torch.Size([14938, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1044, 5])
atom_type_lin's output torch.Size([1044, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
self.norm_1 output  torch.Size([1044, 480])
message in GraphAttention:  torch.Size([14130, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1051, 5])
atom_type_lin's output torch.Size([1051, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
self.norm_1 output  torch.Size([1051, 480])
message in GraphAttention:  torch.Size([14288, 480])
node_atom: tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1059, 5])
atom_type_lin's output torch.Size([1059, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
self.norm_1 output  torch.Size([1059, 480])
message in GraphAttention:  torch.Size([14220, 480])
node_atom: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1147, 5])
atom_type_lin's output torch.Size([1147, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
self.norm_1 output  torch.Size([1147, 480])
message in GraphAttention:  torch.Size([15536, 480])
node_atom: tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1019, 5])
atom_type_lin's output torch.Size([1019, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
self.norm_1 output  torch.Size([1019, 480])
message in GraphAttention:  torch.Size([12904, 480])
node_atom: tensor([1, 2, 2,  ..., 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([1009, 5])
atom_type_lin's output torch.Size([1009, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
self.norm_1 output  torch.Size([1009, 480])
message in GraphAttention:  torch.Size([12404, 480])
node_atom: tensor([1, 3, 1, 2, 3, 1, 2, 1, 1, 0, 0, 0, 0, 1, 3, 1, 2, 3, 1, 3, 2, 1, 0, 0,
        0, 0, 1, 3, 1, 2, 3, 2, 1, 1, 2, 0, 0, 0, 1, 3, 1, 2, 1, 1, 3, 2, 1, 0,
        0, 0, 0, 0, 0, 1, 3, 1, 2, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1,
        1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 2, 2, 2,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0,
        1, 3, 1, 1, 2, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 3, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 2, 3, 1, 2, 2, 0, 0, 0, 0,
        0, 0, 0, 1, 3, 1, 1, 1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1,
        2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 3, 1, 2,
        2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 2, 2, 0,
        0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2,
        3, 2, 1, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 2, 1,
        1, 2, 1, 2, 3, 2, 2, 0, 0, 0, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 0, 0, 0, 2,
        1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 2, 1, 1, 2, 0, 0,
        0, 0, 2, 2, 1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 1, 2, 1,
        0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2,
        2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 1, 1, 3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1,
        1, 3, 2, 2, 1, 3, 0, 0, 0, 2, 1, 1, 2, 1, 3, 3, 2, 2, 0, 0, 2, 1, 1, 2,
        1, 2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1,
        1, 2, 3, 1, 1, 3, 1, 0, 0, 0, 0, 2, 1, 2, 1, 1, 2, 3, 1, 3, 0, 0, 0, 2,
        1, 2, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 2, 2, 0, 0,
        0, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, 2, 2, 2, 0, 0, 0, 2, 1, 2, 1, 1, 1, 3,
        2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 2, 1, 2, 2, 1,
        1, 1, 1, 2, 0, 0, 0, 0, 2, 1, 3, 2, 2, 1, 1, 1, 2, 0, 0, 0, 0, 2, 1, 3,
        1, 2, 3, 1, 3, 2, 0, 0, 0, 2, 1, 3, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0,
        2, 1, 3, 1, 1, 2, 2, 2, 3, 0, 0, 0, 0, 2, 1, 3, 1, 1, 1, 3, 2, 2, 0, 0,
        0, 0, 0, 2, 1, 3, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 3, 2, 1, 1,
        2, 2, 2, 0, 0, 0, 0, 0, 2, 1, 3, 2, 1, 2, 1, 3, 2, 0, 0, 0, 0, 2, 1, 3,
        2, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 3, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0,
        0, 2, 1, 3, 3, 1, 2, 2, 2, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1, 2, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 3, 2, 3, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2,
        2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 3, 2, 2, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,
        2, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 3, 0, 0,
        0, 0, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2,
        1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0,
        0, 2, 1, 1, 3, 2, 2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 1, 3, 2, 2, 3, 1, 3, 0,
        0, 0, 2, 1, 1, 3, 2, 2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([906, 5])
atom_type_lin's output torch.Size([906, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
self.norm_1 output  torch.Size([906, 480])
message in GraphAttention:  torch.Size([10522, 480])
node_atom: tensor([2, 1, 1, 3, 3, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 3, 1, 2, 2, 0,
        0, 0, 0, 0, 2, 1, 1, 3, 1, 3, 3, 2, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2,
        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 2, 1, 1, 1, 1, 3, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1,
        1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 2, 0, 0, 0, 0,
        0, 0, 0, 2, 1, 1, 1, 2, 3, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 2, 2,
        1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1, 3, 2, 3, 0, 0, 0, 0, 0, 2, 1, 1,
        1, 3, 2, 3, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 3, 2, 2, 3, 0, 0, 0, 2,
        1, 1, 2, 1, 1, 3, 2, 2, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
        1, 1, 3, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 1, 1, 3, 0, 0,
        0, 0, 0, 0, 2, 1, 1, 3, 2, 1, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 2,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0,
        2, 1, 2, 1, 2, 1, 3, 2, 3, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 2, 3, 0, 0,
        0, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 3, 2,
        1, 3, 0, 0, 0, 2, 1, 2, 1, 2, 3, 2, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 1,
        1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 2, 1, 2,
        2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 0,
        0, 0, 0, 2, 1, 2, 2, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 2, 1, 2, 2, 2, 2, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 2, 1, 2, 2, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 3, 1, 2, 2,
        0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 3, 1,
        3, 2, 1, 3, 0, 0, 0, 2, 1, 2, 3, 1, 3, 1, 2, 2, 0, 0, 0, 0, 2, 1, 2, 3,
        1, 3, 1, 3, 1, 0, 0, 0, 0, 2, 1, 2, 3, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 2,
        1, 2, 3, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 3, 1, 3, 0,
        0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1,
        2, 2, 2, 3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 2, 1,
        2, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 3, 2, 2, 0,
        0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 1,
        1, 2, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 3, 1, 1, 1, 1,
        2, 1, 1, 2, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 3, 2, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0,
        3, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0, 3, 2, 2, 1, 2, 1, 2, 1, 2, 0, 3, 1, 1,
        1, 1, 2, 3, 2, 1, 0, 0, 3, 1, 1, 2, 1, 1, 1, 2, 3, 0, 0, 3, 1, 1, 2, 1,
        2, 3, 2, 1, 0, 3, 1, 1, 1, 3, 2, 1, 1, 2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 2,
        2, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([895, 5])
atom_type_lin's output torch.Size([895, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
self.norm_1 output  torch.Size([895, 480])
message in GraphAttention:  torch.Size([10390, 480])
node_atom: tensor([3, 1, 1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 2, 2, 0,
        0, 0, 0, 0, 3, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2,
        1, 2, 3, 0, 0, 0, 3, 1, 2, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 2, 2,
        1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 2, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0,
        3, 1, 2, 2, 1, 2, 1, 1, 2, 0, 0, 0, 0, 3, 1, 2, 2, 2, 1, 3, 2, 1, 0, 0,
        3, 1, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 3, 1, 3, 2, 1, 1, 2, 1, 2, 0, 3, 1,
        3, 2, 1, 2, 1, 1, 1, 0, 0, 0, 0, 3, 1, 3, 2, 1, 2, 1, 1, 2, 0, 3, 1, 3,
        2, 1, 2, 1, 1, 3, 0, 0, 3, 1, 1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 3, 1, 1, 1,
        1, 1, 2, 2, 2, 0, 0, 0, 3, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 3, 1, 1,
        1, 1, 2, 2, 2, 1, 0, 0, 0, 3, 1, 1, 3, 1, 1, 1, 2, 3, 0, 0, 0, 3, 1, 1,
        1, 2, 2, 3, 1, 2, 0, 3, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 3, 1, 1, 1,
        3, 2, 1, 1, 3, 0, 0, 0, 3, 1, 1, 2, 1, 2, 3, 1, 1, 0, 0, 3, 1, 1, 2, 1,
        3, 1, 2, 3, 0, 0, 3, 1, 1, 2, 3, 1, 2, 1, 1, 0, 0, 3, 1, 1, 1, 2, 2, 2,
        1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        3, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1,
        2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 3,
        1, 1, 2, 1, 2, 1, 2, 3, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 3, 2, 2, 0, 0,
        0, 0, 3, 1, 1, 3, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 2,
        2, 2, 0, 0, 0, 0, 3, 1, 2, 1, 2, 1, 3, 1, 2, 0, 0, 0, 3, 1, 2, 1, 2, 1,
        3, 2, 2, 0, 0, 3, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 0, 3, 1, 2, 2, 2, 1, 1,
        3, 2, 0, 0, 3, 1, 2, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 3, 1, 2, 1, 1, 2,
        2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 1, 1, 2, 2, 2, 2, 0, 0, 3, 1, 1,
        1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0,
        0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 2, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 1, 2, 2,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 2, 2, 0, 0,
        0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        2, 2, 1, 3, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0,
        0, 0, 3, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 2,
        2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 3, 2, 2, 0, 0, 0, 0, 0, 3, 1,
        1, 2, 1, 2, 3, 1, 2, 0, 0, 0, 3, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0, 0, 3, 1,
        1, 2, 2, 2, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0,
        0, 0, 3, 1, 2, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 3, 1, 2, 2, 1, 1, 2, 1, 2,
        0, 0, 0, 0, 3, 1, 2, 2, 1, 1, 2, 2, 2, 0, 0, 0, 3, 1, 2, 2, 2, 1, 3, 1,
        2, 0, 0, 3, 1, 2, 2, 1, 2, 1, 2, 2, 0, 0, 0, 3, 1, 2, 2, 1, 2, 1, 3, 1,
        0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([845, 5])
atom_type_lin's output torch.Size([845, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
self.norm_1 output  torch.Size([845, 480])
message in GraphAttention:  torch.Size([9380, 480])
node_atom: tensor([3, 1, 2, 2, 1, 3, 1, 2, 1, 0, 0, 0, 0, 0, 3, 1, 2, 2, 1, 2, 1, 2, 2, 0,
        0, 0, 3, 1, 2, 2, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1, 2, 2, 2, 1, 2, 1,
        1, 0, 0, 0, 0, 0, 0, 3, 1, 2, 3, 1, 1, 1, 2, 1, 0, 0, 0, 0, 3, 1, 2, 3,
        1, 3, 1, 3, 1, 0, 0, 0, 3, 1, 2, 3, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 3,
        1, 2, 3, 1, 1, 3, 1, 3, 0, 0, 0, 3, 1, 2, 3, 2, 1, 3, 1, 3, 0, 0, 3, 1,
        1, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 1, 2, 3, 0, 0, 0,
        0, 0, 0, 3, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1,
        1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0,
        0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 2, 2, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        2, 1, 1, 3, 2, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 2, 2, 1, 0, 0, 0,
        0, 0, 0, 3, 1, 1, 3, 1, 1, 3, 2, 1, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 1,
        2, 3, 0, 0, 0, 0, 3, 1, 1, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        3, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 3, 2, 2, 0, 0,
        0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2,
        3, 1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 2, 3, 1, 2, 0, 0, 0, 3, 1, 1, 1,
        3, 2, 2, 1, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 2, 3, 2, 2, 0, 0, 0, 0,
        0, 3, 1, 1, 1, 2, 1, 3, 2, 2, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 2, 2, 1, 2,
        0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1,
        2, 1, 3, 2, 2, 1, 0, 0, 0, 0, 0, 3, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 0, 0,
        0, 3, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 3, 1, 1, 2, 2, 2, 1, 1, 2,
        0, 0, 0, 0, 3, 1, 1, 2, 2, 2, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1, 1, 2, 3, 2,
        1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2,
        1, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2, 2, 2, 0, 0, 0, 0,
        0, 0, 0, 0, 3, 1, 1, 2, 1, 2, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3,
        1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 3, 1, 1, 3, 2, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 3, 1, 1, 2, 3, 3, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 2,
        3, 3, 0, 0, 0, 0, 1, 1, 2, 1, 1, 3, 2, 3, 3, 0, 0, 0, 0, 3, 1, 3, 1, 1,
        3, 2, 3, 3, 0, 2, 1, 1, 3, 1, 1, 2, 3, 3, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1,
        2, 3, 3, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 3, 3, 0, 0, 0, 0, 2, 1, 2,
        2, 1, 1, 2, 3, 3, 0, 0, 0, 0, 2, 1, 2, 2, 2, 1, 2, 3, 3, 0, 0, 0, 2, 1,
        2, 3, 1, 1, 2, 3, 3, 0, 0, 0, 1, 1, 1, 4, 1, 3, 1, 1, 3, 0, 0, 0, 0, 0,
        2, 1, 1, 4, 1, 2, 2, 1, 3, 0, 0, 0, 0, 2, 1, 1, 4, 2, 1, 1, 1, 3, 0, 0,
        0, 0, 0, 1, 1, 1, 4, 1, 3, 3, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 4, 1, 2, 3,
        1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 4, 1, 1, 1, 3, 3, 0, 0, 0, 0, 2, 1, 1, 4,
        1, 2, 1, 3, 3, 0, 0, 0, 4, 1, 1, 4, 2, 1, 1, 2, 2, 0, 0, 0],
       device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([908, 5])
atom_type_lin's output torch.Size([908, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
self.norm_1 output  torch.Size([908, 480])
message in GraphAttention:  torch.Size([10892, 480])
node_atom: tensor([3, 1, 1, 4, 2, 1, 1, 3, 3, 0, 0, 1, 1, 1, 1, 3, 1, 4, 1, 2, 0, 0, 0, 0,
        0, 0, 2, 1, 1, 2, 1, 4, 1, 1, 3, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 4, 1, 1,
        3, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 4, 2, 0, 0, 0, 0, 0, 0, 4, 1, 1, 1,
        3, 3, 1, 2, 3, 0, 0, 4, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 3, 2,
        1, 4, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 3, 3, 1, 4, 2, 0, 0, 0, 2, 1, 1, 1,
        4, 1, 1, 3, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 4, 2, 1, 2, 3, 0, 0, 0, 0, 2,
        1, 2, 1, 4, 2, 1, 2, 3, 0, 0, 0, 4, 1, 2, 1, 4, 2, 1, 2, 2, 0, 0, 1, 1,
        1, 1, 2, 2, 1, 4, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 1, 2, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 4, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1,
        4, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 3, 2, 1, 4, 1, 0, 0, 0, 3,
        1, 2, 1, 4, 2, 1, 3, 2, 0, 0, 1, 1, 1, 2, 1, 4, 1, 3, 3, 0, 0, 0, 0, 2,
        1, 1, 2, 1, 4, 3, 1, 3, 0, 0, 0, 3, 1, 1, 2, 1, 4, 1, 2, 3, 0, 0, 0, 1,
        1, 1, 1, 2, 2, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 4,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 4, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 4, 1,
        3, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 4, 1, 3, 2, 0, 0, 0, 0, 0, 1, 1,
        2, 1, 1, 3, 1, 4, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 3, 1, 1, 1, 4, 0, 0,
        0, 0, 0, 0, 2, 1, 1, 1, 1, 3, 1, 4, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 4,
        2, 1, 4, 0, 0, 0, 0, 2, 1, 1, 2, 1, 2, 1, 4, 2, 0, 0, 0, 0, 0, 2, 1, 1,
        2, 1, 3, 1, 4, 1, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 3, 1, 4, 2, 0, 0, 0, 0,
        2, 1, 1, 2, 1, 4, 1, 3, 2, 0, 0, 0, 0, 2, 1, 1, 2, 1, 4, 2, 1, 2, 0, 0,
        0, 0, 0, 2, 1, 2, 1, 3, 1, 1, 1, 4, 0, 0, 0, 0, 0, 2, 1, 2, 1, 4, 1, 1,
        1, 3, 0, 0, 0, 0, 0, 2, 1, 2, 1, 4, 1, 2, 1, 3, 0, 0, 0, 0, 2, 1, 2, 1,
        3, 2, 1, 1, 4, 0, 0, 0, 0, 2, 1, 2, 1, 1, 4, 1, 4, 1, 0, 0, 0, 0, 3, 1,
        1, 1, 1, 4, 1, 4, 2, 0, 0, 0, 3, 1, 2, 1, 1, 4, 2, 1, 4, 0, 0, 4, 1, 1,
        1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 4, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 4, 1,
        2, 1, 1, 1, 2, 1, 1, 0, 0, 0, 1, 1, 1, 4, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 3, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 1, 3,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 3, 1, 4, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 1, 3, 1, 2,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 1, 1, 1, 1, 4, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 3, 1, 1, 2, 1, 4, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 1, 4, 0, 0,
        0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 4, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 4, 1,
        4, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 4, 1, 2, 0, 0, 0, 0, 0, 0,
        2, 1, 2, 1, 1, 1, 4, 1, 4, 0, 0, 0, 0, 4, 1, 1, 3, 1, 3, 1, 3, 3, 0, 1,
        1, 2, 1, 4, 1, 2, 1, 3, 0, 0, 0, 0, 0, 3, 1, 2, 1, 4, 1, 3, 1, 2, 0, 0,
        0, 1, 1, 2, 1, 3, 2, 1, 1, 4, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([879, 5])
atom_type_lin's output torch.Size([879, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
self.norm_1 output  torch.Size([879, 480])
message in GraphAttention:  torch.Size([10342, 480])
node_atom: tensor([1, 1, 2, 1, 3, 3, 1, 1, 4, 0, 0, 0, 0, 2, 1, 1, 1, 3, 3, 1, 1, 4, 0, 0,
        0, 0, 4, 1, 1, 3, 1, 3, 1, 1, 4, 0, 0, 4, 1, 1, 3, 1, 2, 3, 1, 3, 0, 0,
        4, 1, 1, 3, 1, 3, 2, 1, 2, 0, 0, 0, 4, 1, 1, 3, 1, 1, 4, 1, 3, 0, 0, 1,
        2, 1, 2, 1, 4, 2, 1, 3, 0, 0, 0, 0, 1, 2, 1, 3, 1, 4, 2, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 2, 1, 1, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1,
        1, 4, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 2, 1, 1, 4, 1, 1, 0, 0, 0, 0, 0,
        0, 3, 1, 1, 1, 2, 1, 4, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 4, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 4, 2, 0, 0, 0, 0, 0, 0, 1,
        3, 1, 2, 1, 2, 1, 4, 2, 0, 0, 0, 0, 1, 1, 1, 4, 1, 3, 1, 2, 3, 0, 0, 0,
        0, 1, 1, 1, 4, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 1, 1, 1, 3, 2, 2, 1, 1, 4,
        0, 0, 0, 0, 0, 1, 1, 1, 4, 2, 2, 1, 1, 4, 0, 0, 0, 0, 1, 1, 1, 4, 2, 2,
        3, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 4, 1, 2, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2,
        1, 4, 1, 1, 1, 3, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 1, 4, 2, 0, 0, 0, 0,
        0, 1, 2, 2, 1, 1, 3, 1, 1, 4, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 4, 2, 1, 3,
        0, 0, 0, 0, 1, 2, 2, 2, 1, 3, 1, 1, 4, 0, 0, 0, 0, 1, 2, 1, 3, 2, 1, 4,
        1, 2, 0, 0, 0, 0, 1, 2, 1, 3, 2, 1, 1, 4, 1, 0, 0, 0, 0, 0, 1, 2, 1, 3,
        2, 1, 2, 1, 4, 0, 0, 0, 0, 2, 1, 1, 3, 1, 2, 2, 1, 4, 0, 0, 0, 0, 2, 1,
        1, 4, 1, 2, 3, 1, 2, 0, 0, 0, 0, 2, 1, 1, 4, 2, 2, 3, 1, 3, 0, 0, 2, 1,
        1, 1, 3, 2, 2, 1, 4, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 4, 1, 2, 0, 0, 0, 0,
        0, 2, 1, 2, 3, 1, 4, 1, 1, 2, 0, 0, 0, 0, 2, 1, 2, 3, 1, 4, 2, 1, 2, 0,
        0, 0, 2, 1, 2, 3, 2, 1, 4, 1, 2, 0, 0, 0, 4, 1, 1, 4, 1, 3, 1, 2, 3, 0,
        4, 1, 1, 1, 2, 1, 4, 2, 3, 0, 0, 4, 1, 1, 1, 2, 3, 2, 1, 4, 0, 0, 4, 1,
        2, 2, 1, 3, 1, 2, 2, 0, 0, 0, 4, 1, 2, 3, 1, 2, 1, 3, 3, 0, 4, 1, 1, 1,
        4, 2, 3, 1, 2, 0, 0, 4, 1, 2, 3, 1, 3, 2, 1, 2, 0, 0, 4, 1, 4, 4, 1, 1,
        1, 1, 2, 0, 0, 0, 0, 1, 1, 3, 1, 3, 1, 4, 4, 4, 0, 0, 0, 2, 1, 3, 3, 1,
        1, 4, 4, 4, 0, 0, 0, 0, 4, 1, 4, 4, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 4, 1,
        4, 4, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0,
        0, 0, 4, 1, 4, 4, 1, 1, 1, 1, 2, 0, 0, 0, 0, 4, 1, 4, 4, 1, 1, 3, 1, 2,
        0, 0, 4, 1, 4, 4, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 4, 1, 1,
        1, 1, 2, 0, 0, 0, 0, 0, 0, 4, 1, 4, 4, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0,
        4, 1, 4, 4, 1, 2, 1, 1, 3, 0, 0, 0, 0, 4, 1, 4, 4, 1, 2, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 4, 1, 4, 4, 1, 3, 1, 1, 3, 0, 0, 0, 4, 1, 4, 4, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 4, 4, 4, 1, 3, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 4,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 4, 1, 3, 1, 1, 2, 0, 0, 0,
        0, 1, 2, 1, 1, 3, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 3, 1, 4, 4,
        4, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([848, 5])
atom_type_lin's output torch.Size([848, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
self.norm_1 output  torch.Size([848, 480])
message in GraphAttention:  torch.Size([9858, 480])
node_atom: tensor([1, 1, 1, 3, 1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 4,
        4, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 4, 4, 4, 1, 3, 0, 0, 0, 0, 0,
        0, 1, 1, 2, 1, 3, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 4, 4,
        4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 2, 1, 1, 1, 3, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 4, 4, 4, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 1,
        1, 1, 1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 4, 4, 4, 0, 0, 0,
        0, 0, 3, 1, 1, 2, 1, 1, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1,
        1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
node_atom_onehot: tensor([[0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        ...,
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.]], device='cuda:0')
atom_type_lin's input torch.Size([249, 5])
atom_type_lin's output torch.Size([249, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])
self.norm_1 output  torch.Size([249, 480])
message in GraphAttention:  torch.Size([3622, 480])

==================================================
推理结果:
  MAE: 0.05723
  预测值范围: [-112.974, -8.672]
  真实值范围: [-113.004, -9.240]
==================================================
